{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_NJMM3xM5i5"
      },
      "source": [
        "Created on Sat Dec  9 19:24:25 2023\n",
        "@author: Michelle Fribance\n",
        "\n",
        "The purpose of this script is to train a Markov Chain model on the combined\n",
        "fortunes dataset, using the markovify library: https://github.com/jsvine/markovify\n",
        "then generate a set of unique fortunes for evaluation purposes. A trained model\n",
        "is then savedas a pickle file, to be used by the fortune_gui.py program.\n",
        "\n",
        "If you don't have pickle files yet (the pretrained Markov models), then adjust the\n",
        "paths on lines 60 and 64 to run this script and create them.\n",
        "\n",
        "markovify isn't available through Anaconda; must install using pip on your desired env:\n",
        "pip install markovify\n",
        "\n",
        "Key parameters of the markovify library:\n",
        "state_size (default=2):\n",
        "    - Determines the number of words that form the state of the Markov Chain.\n",
        "    - Larger values generally lead to more coherent but less diverse text.\n",
        "\n",
        "chain (default=None):\n",
        "    - If you have a pre-built chain (possibly from a previous run), you can\n",
        "      provide it to the model using this parameter.\n",
        "  eg: chain = markovify.Chain.from_text(\" \".join(proverbs), state_size=2)\n",
        "      text_model = markovify.Text(\"\", chain=chain)\n",
        "\n",
        "max_overlap_ratio (default=0.7):\n",
        "    - This parameter controls the maximum allowed ratio of overlapping words\n",
        "      between the sentences.\n",
        "\n",
        "max_overlap_total (default=15):\n",
        "    - This parameter controls the maximum allowed total number of overlapping\n",
        "      words between the sentences.\n",
        "\n",
        "output_max_words (default=200):\n",
        "    - Maximum number of words in the generated sentence.\n",
        "\n",
        "tries (default=10):\n",
        "    - The number of attempts to make a sentence before failing and retrying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSItagD-WUHA"
      },
      "outputs": [],
      "source": [
        "pip install markovify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9A_EpQ-Y17T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import markovify\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.lm import MLE\n",
        "import pickle\n",
        "\n",
        "# Check if punkt is already downloaded\n",
        "try:\n",
        "    # Check if punkt is found\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    # If not found, download punkt\n",
        "    nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGU_zT5NY6ch",
        "outputId": "5049d6bc-a2b7-4dc6-8b74-0f8b71a9724b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number of retraining rounds set to 25\n",
            "\n",
            "Seed value set: 42\n"
          ]
        }
      ],
      "source": [
        "# --------------------------- Set model parameters --------------------------- #\n",
        "\n",
        "# Set markovify model parameters:\n",
        "state_size = 3\n",
        "\n",
        "# Set number of rounds for retraining:\n",
        "num_rounds = 25\n",
        "print(f\"\\nNumber of retraining rounds set to {num_rounds}\")\n",
        "\n",
        "# Set a seed value for reproducibility\n",
        "seed_value = 42\n",
        "random.seed(seed_value)  # Sets the seed for the Python random number generator\n",
        "print(f\"\\nSeed value set: {seed_value}\")\n",
        "\n",
        "num_fortunes_to_generate = 100\n",
        "tries = 100 # Attemps by the Markov model to generate a fortune before starting over\n",
        "\n",
        "# Set whether or not to filter dissimilar generated fortunes by cosine similarity:\n",
        "cosine_sim = \"false\"\n",
        "\n",
        "# Set the display.max_colwidth option to None to show full text in a column\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRilA-ThUdBD"
      },
      "outputs": [],
      "source": [
        "############################ Function definitions #############################\n",
        "\n",
        "def filter_fortunes_with_cosine_similarity(df_generated_fortunes, original_fortunes):\n",
        "    \"\"\" Removes fortunes with too-low similarity to the training set. For word\n",
        "        embeddings using spaCy, we use the pre-trained spaCy model\n",
        "        \"en_core_web_md\" (medium-sized English model). This model includes word vectors,\n",
        "        and it should work well for general-purpose applications, including fortunes.\"\"\"\n",
        "\n",
        "    # Load the model:\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_md\")\n",
        "    except OSError:\n",
        "        print(\"Downloading 'en_core_web_md' model...\")\n",
        "        spacy.cli.download(\"en_core_web_md\")\n",
        "        nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "    # Tokenize the training fortunes:\n",
        "    training_tokens = [nlp(fortune) for fortune in original_fortunes]\n",
        "\n",
        "    # Calculate the average vector of all training fortunes:\n",
        "    average_training_vector = np.mean([token.vector for tokens in training_tokens for token in tokens],\n",
        "                                      axis=0)\n",
        "\n",
        "    # Function to calculate cosine similarity with the average training vector:\n",
        "    def calculate_cosine_similarity_with_average(text):\n",
        "        if text is None:\n",
        "            return 0.0\n",
        "        tokens = nlp(text)\n",
        "        vector = np.mean([token.vector for token in tokens], axis=0)\n",
        "        similarity = cosine_similarity([average_training_vector], [vector])[0][0]\n",
        "        return similarity\n",
        "\n",
        "    # Apply the cosine similarity function while handling None values\n",
        "    df_generated_fortunes[\"Passes_Threshold\"] = df_generated_fortunes[\"Generated fortunes\"].apply(\n",
        "        lambda x: calculate_cosine_similarity_with_average(x) if x is not None else 0.0)\n",
        "\n",
        "    # Filter out rows with None values in the \"Generated fortunes\" column\n",
        "    df_generated_fortunes = df_generated_fortunes.dropna(subset=[\"Generated fortunes\"])\n",
        "\n",
        "    cosine_similarity_threshold = 0.7\n",
        "    df_generated_fortunes = df_generated_fortunes.copy()\n",
        "    df_generated_fortunes.loc[:, \"Passes_Threshold\"] = (df_generated_fortunes[\"Passes_Threshold\"] >= cosine_similarity_threshold)\n",
        "\n",
        "    # Filter out generated fortunes below the threshold:\n",
        "    filtered_fortunes = df_generated_fortunes[df_generated_fortunes[\"Passes_Threshold\"]]\n",
        "\n",
        "    # Print the removed fortunes:\n",
        "    #removed_fortunes = df_generated_fortunes[~df_generated_fortunes[\"Passes_Threshold\"]]\n",
        "    #print(f\"\\nNumber of unique fortunes removed: {len(removed_fortunes)}\")\n",
        "    #print(\"Removed fortunes:\")\n",
        "    #print(removed_fortunes[[\"Generated fortunes\", \"Passes_Threshold\"]])\n",
        "\n",
        "    # Print the remaining filtered fortunes:\n",
        "    #print(f\"\\nNumber of unique fortunes in filtered_fortunes: {len(filtered_fortunes)}\")\n",
        "    #print(\"Remaining filtered fortunes:\")\n",
        "    #print(filtered_fortunes[[\"Generated fortunes\", \"Passes_Threshold\"]])\n",
        "\n",
        "    # Drop the temporary column\n",
        "    filtered_fortunes = filtered_fortunes.drop(columns=[\"Passes_Threshold\"])\n",
        "\n",
        "    return filtered_fortunes\n",
        "\n",
        "\n",
        "def evaluate_generated_fortunes(filtered_fortunes, original_fortunes):\n",
        "    # Evaluate by calculating perplexity for each fortune\n",
        "\n",
        "    # https://www.nltk.org/api/nltk.lm.html\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_md\")  # Load pre-trained spaCy model with word vectors\n",
        "\n",
        "    # Tokenize training fortunes and pad with special characters at each sequence's start & end\n",
        "    train_data = [nlp(sentence) for sentence in original_fortunes]\n",
        "    vocabulary = [token.text for tokens in train_data for token in tokens]\n",
        "\n",
        "    # Create an NLTK model for the reference data:\n",
        "    n = 2\n",
        "    nltk_model = MLE(n)\n",
        "\n",
        "    # Convert training data into n-grams\n",
        "    train_ngrams = list(nltk.everygrams(vocabulary, max_len=n))\n",
        "\n",
        "    # Fit the model\n",
        "    nltk_model.fit([train_ngrams], vocabulary_text=vocabulary)\n",
        "\n",
        "    # Define a function to calculate perplexity for a given sentence using the trained NLTK model:\n",
        "    def calculate_perplexity(sentence, model, n):\n",
        "        if sentence is None:\n",
        "            return float('inf')  # Return infinity for None values\n",
        "        tokens = nlp(sentence)\n",
        "        ngrams = list(nltk.everygrams(tuple([token.text for token in tokens]), max_len=n))\n",
        "        return model.perplexity(ngrams)\n",
        "\n",
        "    # Add a new column to the DataFrame to store perplexity values\n",
        "    filtered_fortunes[\"Perplexity\"] = filtered_fortunes[\"Generated fortunes\"].apply(\n",
        "        lambda x: calculate_perplexity(x, nltk_model, n))\n",
        "\n",
        "    # Sort the dataframe by perplexity in ascending order (lower perplexity is better):\n",
        "    filtered_fortunes = filtered_fortunes.sort_values(by=\"Perplexity\", ascending=True)\n",
        "\n",
        "    #print(filtered_fortunes[[\"Generated fortunes\", \"Perplexity\"]])\n",
        "\n",
        "    # Check for duplicates and remove:\n",
        "    number_of_duplicates = len(filtered_fortunes) - len(filtered_fortunes.drop_duplicates())\n",
        "\n",
        "    if number_of_duplicates > 0:\n",
        "        #duplicates = filtered_fortunes[filtered_fortunes.duplicated()]\n",
        "        filtered_fortunes = filtered_fortunes.drop_duplicates()\n",
        "        #print(f\"\\n{number_of_duplicates} duplicate fortunes removed from filtered_fortunes. Duplicates: \")\n",
        "        #print(duplicates)\n",
        "    #else:\n",
        "        #print(\"\\nNo duplicates found in filtered_fortunes.\\n\")\n",
        "\n",
        "    # Print the top fortunes:\n",
        "    #top_n = 10\n",
        "    #print(f\"\\nTop {top_n} generated fortunes (based on lower perplexity being better):\")\n",
        "    #print(filtered_fortunes.head(top_n)[[\"Generated fortunes\", \"Perplexity\"]])\n",
        "\n",
        "    # Filter out rows with \"inf\" perplexity:\n",
        "    valid_perplexity_df = filtered_fortunes[filtered_fortunes[\"Perplexity\"] != float('inf')]\n",
        "\n",
        "    return valid_perplexity_df\n",
        "\n",
        "\n",
        "def generate_fortune():\n",
        "    # Load the trained Markov model from the saved file\n",
        "    with open(\"trained_markov_model-state_size_3.pkl\", \"rb\") as f:\n",
        "        text_model = pickle.load(f)\n",
        "\n",
        "    # Generate a single fortune\n",
        "    try:\n",
        "        print(\"Generating fortune...\")\n",
        "        generated_fortune = text_model.make_sentence(\n",
        "            max_words=15, max_overlap_ratio=0.5, tries=100)\n",
        "        return generated_fortune\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating fortune: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "######################## End of Function Definitions ##########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a9LQImaZQ2v",
        "outputId": "4e9a6aa7-a9ff-411e-8992-ee693cca4442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Markov model built using state size 3\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------- Load data -------------------------------- #\n",
        "\n",
        "# Open the original combined_fortunes dataset\n",
        "training_fortunes_path = os.path.join('..', 'datasets', 'combined_fortunes-4632.csv')\n",
        "\n",
        "\n",
        "# Set the original dataset as the training fortunes for the first round\n",
        "with open(training_fortunes_path, 'r') as file:\n",
        "    training_fortunes = file.readlines()\n",
        "\n",
        "\n",
        "previous_fortunes = training_fortunes # Define this here for later\n",
        "\n",
        "\n",
        "# ----------------- Build the initial Markov Chain model -------------------- #\n",
        "\n",
        "# Combine the fortunes into a single string for Markovify\n",
        "text_model = markovify.Text(\" \".join(training_fortunes), state_size=state_size)\n",
        "print(f\"\\nMarkov model built using state size {state_size}\")\n",
        "\n",
        "# Initialize dataframe\n",
        "df_generated_fortunes = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsO4nsX8ZARu",
        "outputId": "6e3ec33f-f303-4b9e-aae6-f363bc9a8fb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 0/1000\n",
            "Downloading 'en_core_web_md' model...\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\n",
            "Round 1: Average Perplexity for all generated fortunes: 95.93937701922252\n",
            "Number of fortunes in expanded fortunes dataset:  4765\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 1/1000\n",
            "\n",
            "Round 2: Average Perplexity for all generated fortunes: 95.91622284591969\n",
            "Number of fortunes in expanded fortunes dataset:  4865\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 2/1000\n",
            "\n",
            "Round 3: Average Perplexity for all generated fortunes: 94.68340087880868\n",
            "Number of fortunes in expanded fortunes dataset:  4959\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 4/1000\n",
            "\n",
            "Round 4: Average Perplexity for all generated fortunes: 97.99066993401381\n",
            "Number of fortunes in expanded fortunes dataset:  5051\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 6/1000\n",
            "\n",
            "Round 5: Average Perplexity for all generated fortunes: 94.76325009222751\n",
            "Number of fortunes in expanded fortunes dataset:  5141\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 5/1000\n",
            "\n",
            "Round 6: Average Perplexity for all generated fortunes: 93.27923722735558\n",
            "Number of fortunes in expanded fortunes dataset:  5239\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 2/1000\n",
            "\n",
            "Round 7: Average Perplexity for all generated fortunes: 93.83860399477305\n",
            "Number of fortunes in expanded fortunes dataset:  5334\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 6/1000\n",
            "\n",
            "Round 8: Average Perplexity for all generated fortunes: 93.64905700880104\n",
            "Number of fortunes in expanded fortunes dataset:  5427\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 4/1000\n",
            "\n",
            "Round 9: Average Perplexity for all generated fortunes: 95.61789394867334\n",
            "Number of fortunes in expanded fortunes dataset:  5517\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 10/1000\n",
            "\n",
            "Round 10: Average Perplexity for all generated fortunes: 94.89989987514537\n",
            "Number of fortunes in expanded fortunes dataset:  5607\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 13/1000\n",
            "\n",
            "Round 11: Average Perplexity for all generated fortunes: 98.13502693234885\n",
            "Number of fortunes in expanded fortunes dataset:  5697\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 16/1000\n",
            "\n",
            "Round 12: Average Perplexity for all generated fortunes: 97.96226462524288\n",
            "Number of fortunes in expanded fortunes dataset:  5783\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 15/1000\n",
            "\n",
            "Round 13: Average Perplexity for all generated fortunes: 101.43233805720152\n",
            "Number of fortunes in expanded fortunes dataset:  5870\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 29/1000\n",
            "\n",
            "Round 14: Average Perplexity for all generated fortunes: 102.82314222363607\n",
            "Number of fortunes in expanded fortunes dataset:  5950\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 26/1000\n",
            "\n",
            "Round 15: Average Perplexity for all generated fortunes: 104.9896060191044\n",
            "Number of fortunes in expanded fortunes dataset:  6028\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 34/1000\n",
            "\n",
            "Round 16: Average Perplexity for all generated fortunes: 104.86713836084229\n",
            "Number of fortunes in expanded fortunes dataset:  6105\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 36/1000\n",
            "\n",
            "Round 17: Average Perplexity for all generated fortunes: 111.13463257511016\n",
            "Number of fortunes in expanded fortunes dataset:  6177\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 46/1000\n",
            "\n",
            "Round 18: Average Perplexity for all generated fortunes: 111.90456992934342\n",
            "Number of fortunes in expanded fortunes dataset:  6243\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 65/1000\n",
            "\n",
            "Round 19: Average Perplexity for all generated fortunes: 115.98772398935897\n",
            "Number of fortunes in expanded fortunes dataset:  6309\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 70/1000\n",
            "\n",
            "Round 20: Average Perplexity for all generated fortunes: 117.84813113300977\n",
            "Number of fortunes in expanded fortunes dataset:  6369\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 70/1000\n",
            "\n",
            "Round 21: Average Perplexity for all generated fortunes: 116.63134124582845\n",
            "Number of fortunes in expanded fortunes dataset:  6423\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 70/1000\n",
            "\n",
            "Round 22: Average Perplexity for all generated fortunes: 121.71249998113382\n",
            "Number of fortunes in expanded fortunes dataset:  6475\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 110/1000\n",
            "\n",
            "Round 23: Average Perplexity for all generated fortunes: 123.35287766132126\n",
            "Number of fortunes in expanded fortunes dataset:  6521\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 112/1000\n",
            "\n",
            "Round 24: Average Perplexity for all generated fortunes: 127.01228967653395\n",
            "Number of fortunes in expanded fortunes dataset:  6562\n",
            "Generating fortunes...\n",
            "\n",
            "Failed attempts to generate a fortune: 106/1000\n",
            "\n",
            "Round 25: Average Perplexity for all generated fortunes: 128.8042813324913\n",
            "Number of fortunes in expanded fortunes dataset:  6599\n"
          ]
        }
      ],
      "source": [
        "##################### Retrain the model in several rounds #####################\n",
        "\n",
        "for i in range(1, num_rounds + 1):\n",
        "\n",
        "    # Generate fortunes and save to a DataFrame:\n",
        "    print(\"Generating fortunes...\")\n",
        "    generated_fortunes = [text_model.make_sentence(max_words=15, max_overlap_ratio=0.5,\n",
        "                            tries=tries) for _ in range(num_fortunes_to_generate)]\n",
        "\n",
        "    # If df already exists (ie, all retraining rounds), then just overwrite that df:\n",
        "    if df_generated_fortunes is not None and \"Generated fortunes\" in df_generated_fortunes.columns:\n",
        "        # Check if the length of the generated fortunes list matches the length of the DataFrame\n",
        "        if len(generated_fortunes) != len(df_generated_fortunes):\n",
        "            # Adjust the size of the generated fortunes list to match the length of the DataFrame\n",
        "            generated_fortunes = generated_fortunes[:len(df_generated_fortunes)]\n",
        "        # Overwrite the existing column with the new generated fortunes\n",
        "        df_generated_fortunes[\"Generated fortunes\"] = generated_fortunes\n",
        "    else:\n",
        "        # If df_generated_fortunes doesn't exist yet or doesn't have the column,\n",
        "        # create a new DataFrame with the generated fortunes column\n",
        "        df_generated_fortunes = pd.DataFrame({\"Generated fortunes\": generated_fortunes})\n",
        "\n",
        "    # Remove blank fortunes:\n",
        "    df_generated_fortunes = df_generated_fortunes.dropna(how='all')\n",
        "\n",
        "\n",
        "    # ----------- Check for failed or duplicate generated fortunes -------------- #\n",
        "\n",
        "    # Print the count of null values (number of failed attempts of the Markov model to generate a fortune):\n",
        "    print(f\"\\nFailed attempts to generate a fortune: {df_generated_fortunes.isnull().sum()['Generated fortunes']}/{num_fortunes_to_generate}\")\n",
        "\n",
        "    # Remove blank lines (failed attempts at generating a fortune):\n",
        "    df_generated_fortunes = df_generated_fortunes.dropna(how='all')\n",
        "\n",
        "    # Check for duplicates and remove:\n",
        "    number_of_duplicates = len(df_generated_fortunes) - len(df_generated_fortunes.drop_duplicates())\n",
        "\n",
        "    if number_of_duplicates > 0:\n",
        "        #duplicates = df_generated_fortunes[df_generated_fortunes.duplicated()]\n",
        "        df_generated_fortunes = df_generated_fortunes.drop_duplicates()\n",
        "        #print(f\"\\n{number_of_duplicates} duplicate fortunes removed from df_generated_fortunes. Duplicates: \")\n",
        "        #print(duplicates)\n",
        "    #else:\n",
        "        #print(\"\\nNo duplicates found in df_generated_fortunes.\\n\")\n",
        "\n",
        "    #print(f\"\\nNumber of unique fortunes in df_generated_fortunes: {len(df_generated_fortunes)}\")\n",
        "\n",
        "    # Print several successful fortunes for manual inspection:\n",
        "    #print(df_generated_fortunes[:5])\n",
        "\n",
        "\n",
        "    # Filter bad fortunes using Cosine Similarity / Word Embeddings:\n",
        "    if cosine_sim == \"true\":\n",
        "        # Filter out fortunes below the threshold\n",
        "        filtered_fortunes = filter_fortunes_with_cosine_similarity(df_generated_fortunes, previous_fortunes)\n",
        "    else:\n",
        "        filtered_fortunes = df_generated_fortunes\n",
        "\n",
        "\n",
        "    # -------- Evaluate generated fortunes by calculating Perplexity ------- #\n",
        "\n",
        "    valid_perplexity_df = evaluate_generated_fortunes(filtered_fortunes, previous_fortunes)\n",
        "\n",
        "\n",
        "    # Calculate the average perplexity for all generated sentences:\n",
        "    average_perplexity = valid_perplexity_df[\"Perplexity\"].mean()\n",
        "    print(f\"\\nRound {i}: Average Perplexity for all generated fortunes: {average_perplexity}\")\n",
        "\n",
        "    # Extract the \"Generated fortunes\" column and convert it to a list:\n",
        "    generated_fortunes_list = valid_perplexity_df[\"Generated fortunes\"].tolist()\n",
        "\n",
        "\n",
        "    # ------- Combine top generated fortunes with original dataset ---------- #\n",
        "\n",
        "    # Calculate the number of items to keep (20% of the length of generated_fortunes_list)\n",
        "    num_to_keep = int(len(generated_fortunes_list) * 0.2)\n",
        "\n",
        "    # Keep the first num_to_keep items\n",
        "    expanded_fortunes = previous_fortunes + generated_fortunes_list[:num_to_keep]\n",
        "    print(\"Number of fortunes in expanded fortunes dataset: \", len(expanded_fortunes))\n",
        "    previous_fortunes = expanded_fortunes  # Define this here for the next round of training\n",
        "\n",
        "\n",
        "    # ------------ Retrain the model on the expanded dataset ---------------- #\n",
        "\n",
        "    # Join the expanded fortunes with a space and retrain the model\n",
        "    text_model = markovify.Text(\" \".join(expanded_fortunes), state_size=state_size)\n",
        "\n",
        "\n",
        "# Save the final text_model to be used by the fortune_gui.py program:\n",
        "with open(f\"trained_markov_model-state_size_{state_size}.pkl\", \"wb\") as f:\n",
        "    pickle.dump(text_model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FobXoH-uZWqR",
        "outputId": "ab88d299-3803-4d14-e169-2459d21e66c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated fortunes dataset exported to: Generated_Fortunes/generated_fortunes-state_size_3-rounds_25.csv\n",
            "\n",
            "Expanded fortunes dataset exported to: Generated_Fortunes/expanded_fortunes-state_size_3-rounds_25.csv\n"
          ]
        }
      ],
      "source": [
        "############################## Export Datasets ################################\n",
        "\n",
        "# Check for or create \"Generated_Fortunes\" folder, to store results:\n",
        "generated_fortunes_folder = \"Generated_Fortunes\"\n",
        "if not os.path.exists(generated_fortunes_folder):\n",
        "    os.makedirs(generated_fortunes_folder)\n",
        "\n",
        "\n",
        "# --------- Export generated fortunes to CSV for manual evaluation ---------- #\n",
        "\n",
        "# Export the generated DataFrame to a CSV file:\n",
        "csv_file_path = os.path.join(generated_fortunes_folder,\n",
        "                              f\"generated_fortunes-state_size_{state_size}-rounds_{num_rounds}.csv\")\n",
        "valid_perplexity_df.to_csv(csv_file_path, index=False)\n",
        "print(f\"\\nGenerated fortunes dataset exported to: {csv_file_path}\")\n",
        "\n",
        "\n",
        "# ----- Export combined fortunes to CSV for use by final Gui program -------- #\n",
        "\n",
        "# Create DataFrame out of original dataset and generated fortunes:\n",
        "expanded_fortunes_df = pd.DataFrame({\"Generated fortunes\": expanded_fortunes})\n",
        "\n",
        "# Export the expanded DataFrame to a CSV file for final use by the Gui:\n",
        "csv_file_path = os.path.join(generated_fortunes_folder,\n",
        "                              f\"expanded_fortunes-state_size_{state_size}-rounds_{num_rounds}.csv\")\n",
        "\n",
        "expanded_fortunes_df.to_csv(csv_file_path, index=False)\n",
        "print(f\"\\nExpanded fortunes dataset exported to: {csv_file_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
